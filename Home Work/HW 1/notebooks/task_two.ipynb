{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrmAAbJE9y-W"
      },
      "source": [
        "# Домашняя работа часть 2  (10 баллов): совмещение спарсификации и квантизации.\n",
        "\n",
        "В это задании мы предлагаем вам самим реализовать функции квантизации и спарсисикации и далее совместить их вместе. В качестве базовых методов предлагаем взять Magnitude Pruning и симметричную поканальную RTN квантизацию (коэффициент квантизации выбирается для каждого столбца). Также возможны альтерантивные варианты. Например, можно объединить Wanda Pruning и RTN, GPTQ и SparseGPT, или использование других методов.\n",
        "\n",
        "Результат работы - это метрики L1 и L2  для $|C(\\mathbf{W})\\mathbf{X}^T - \\mathbf{W}\\mathbf{X}^T|$ для каждого слоя сжатого в два раза, количество различных значений весов в каждом столбце матрицы $C(\\mathbf{W})$ и общее количество нулевых значений в этой матрице, где $C$  - функция одновременной спарсификации и квантизации. При этом считаем, что понижение битовости в два раза соотвествует сжатию в два раза, зануление половины весов так же соответствует сжатию в два раза.\n",
        "\n",
        "Чтобы упросить задание, мы не будем квантизовать всю модель, а квантизуем только некоторые слои.\n",
        "\n",
        "В папке llama7b_weights содержатся веса для одоного слоя LLaMа7b и сопутствующие активации в папке llama7b_act_scales (аггрегировые по датасету)\n",
        "\n",
        "В качестве результата нужно будет сдать ноутбук с решением и метриками.\n",
        "\n",
        "### Баллы\n",
        "\n",
        "Релизация алгортима спарсификации - 2 балла <br>\n",
        "Релизация алгортима квантизации - 2 балла <br>\n",
        "Объеденение двух алгоритмов - 6 баллов <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_DbYihup9y-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab50451b-4e46-49b0-e6e1-7cde7c2e2238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gdown 5.2.0\n",
            "Uninstalling gdown-5.2.0:\n",
            "  Successfully uninstalled gdown-5.2.0\n",
            "Collecting gdown\n",
            "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-5.2.0\n",
            "gdown 5.2.0 at /usr/local/lib/python3.10/dist-packages\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall gdown -y && pip install gdown\n",
        "!gdown -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KEFioWPF9y-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0d9bc4-f41d-4795-adae-02577328b654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1n2zThYW1MXqIIPdmQTn1-ANSDhSpLArx\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1n2zThYW1MXqIIPdmQTn1-ANSDhSpLArx&confirm=t&uuid=af1ffd40-7aa4-41af-a73b-5a28d79e5543\n",
            "To: /content/llama7b_weights.zip\n",
            "100%|██████████| 311M/311M [00:07<00:00, 44.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama7b_weights.zip\n",
            "replace llama7b_weights/model.layers.20.mlp.down_proj.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: llama7b_weights/model.layers.20.mlp.down_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.self_attn.v_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.mlp.gate_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.self_attn.k_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.mlp.up_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.self_attn.q_proj.pt  \n",
            "  inflating: llama7b_weights/model.layers.20.self_attn.o_proj.pt  \n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "#Загрузка весов модели\n",
        "url = 'https://drive.google.com/uc?export=download&id=1n2zThYW1MXqIIPdmQTn1-ANSDhSpLArx'\n",
        "output = 'llama7b_weights.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip llama7b_weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gHFGQEcR9y-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7032860f-7c56-463d-d7f4-d35de79cb27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1VnJbohhzLYAP4X1OAHz3gyagj-T7vOKl\n",
            "To: /content/llama7b_act_scales.zip\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 12.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama7b_act_scales.zip\n",
            "replace llama7b_act_scales/Llama-2-7b-hf.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: llama7b_act_scales/Llama-2-7b-hf.pt  \n"
          ]
        }
      ],
      "source": [
        "#Загрузка усредненных активаций\n",
        "url = 'https://drive.google.com/uc?export=download&id=1VnJbohhzLYAP4X1OAHz3gyagj-T7vOKl'\n",
        "output = 'llama7b_act_scales.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip llama7b_act_scales.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8P4dpfy29y-Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r3VjwRLo9y-Z",
        "outputId": "0ace03de-2ec8-4da9-8200-ddec6c4eccd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-5752a9a2f6fe>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[names[1]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4096, 11008])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load weights\n",
        "\n",
        "FOLDER = \"llama7b_weights/\"\n",
        "names = os.listdir(FOLDER)\n",
        "weight_paths =  {name.replace(\".pt\",'') :os.path.join(FOLDER, name) for name in names}\n",
        "names = [name.replace(\".pt\",'') for name in names]\n",
        "W = torch.load(weight_paths[names[1]])\n",
        "W.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")['model.layers.0.self_attn.q_proj'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dADoRDkGLYs0",
        "outputId": "c966134f-a16e-436b-bf62-9faa9d90ff9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-5c754a4e4ef8>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")['model.layers.0.self_attn.q_proj'].shape\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4096])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "3aQHwYI89y-a"
      },
      "outputs": [],
      "source": [
        "# Пример расчета матрицы Гессе H для выбранного слоя,\n",
        "#которая пригодится если реализовывать GPTQ, SparseGPT\n",
        "X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[names[1]]\n",
        "\n",
        "H = torch.outer(X, X)\n",
        "\n",
        "damp = 0.01 * torch.mean(torch.diag(H))\n",
        "diag = torch.arange(X.shape[0])\n",
        "H[diag, diag] += damp\n",
        "\n",
        "H = torch.linalg.cholesky(H)\n",
        "H = torch.cholesky_inverse(H)\n",
        "H = torch.linalg.cholesky(H, upper=True)\n",
        "Hinv = H\n",
        "H.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0nJPSK7KdP2",
        "outputId": "c7bb3b80-78d5-46ca-ee70-cdf68aa7cf55"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11008])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS6xyG_W9y-a"
      },
      "source": [
        "Для вдохновения рекомендуем воспользоваться кодом QUIK\n",
        "\n",
        "https://github.com/IST-DASLab/QUIK/blob/9558d7121c698174fc93940e0b52c38c746c97ea/experiments/quik_utils.py#L80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7ZuJPP9y-a"
      },
      "source": [
        "# Ваше решение тут ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def magnitude_pruning(tensor, *args, prune_rate=0.25):\n",
        "  assert 0 < prune_rate <= 1\n",
        "  prune_count = int(tensor.numel() * prune_rate)\n",
        "\n",
        "  new_tensor = tensor.clone()\n",
        "  thrs = torch.topk(tensor.abs().flatten(), prune_count, largest=False).values.max()\n",
        "  mask = new_tensor.abs() > thrs\n",
        "  return new_tensor * mask, prune_rate * 100"
      ],
      "metadata": {
        "id": "kGTQKfsvR81O"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_Hinv(X):\n",
        "  H = torch.outer(X, X)\n",
        "\n",
        "  damp = 0.01 * torch.mean(torch.diag(H))\n",
        "  diag = torch.arange(X.shape[0])\n",
        "  H[diag, diag] += damp\n",
        "\n",
        "  H = torch.linalg.cholesky(H)\n",
        "  H = torch.cholesky_inverse(H)\n",
        "  H = torch.linalg.cholesky(H, upper=True)\n",
        "  Hinv = H\n",
        "  return Hinv\n",
        "\n",
        "def _quant_tensor(tensor, bitwidth):\n",
        "  max_q = 2**(bitwidth - 1) - 1\n",
        "  min_q = -2**(bitwidth - 1)\n",
        "  scale = (tensor.max() - tensor.min()) / (max_q - min_q)\n",
        "  return scale * torch.clip(tensor / scale, min=min_q, max=max_q).long()\n",
        "\n",
        "\n",
        "def gpqt_quantization(W_init, X, block_size=128, bitwidth=4):\n",
        "  W = W_init.clone()\n",
        "  Hinv = _compute_Hinv(X)\n",
        "  Q = torch.zeros_like(W)\n",
        "\n",
        "  n_cols = W.shape[1]\n",
        "  for i1 in range(0, n_cols, block_size):\n",
        "    i2 = min(i1 + block_size, n_cols)\n",
        "\n",
        "    W_block = W[:, i1:i2].clone()\n",
        "    Q_block = torch.zeros_like(W_block)\n",
        "    Err_block = torch.zeros_like(W_block)\n",
        "    Hinv_block = Hinv[i1:i2, i1:i2]\n",
        "\n",
        "    for j in range(i2 - i1):\n",
        "        w_j = W_block[:, j]\n",
        "        hinv_j = Hinv_block[j, j]\n",
        "\n",
        "        # quant\n",
        "        w_j_quant = _quant_tensor(w_j, bitwidth).flatten()\n",
        "\n",
        "        Q_block[:, j] = w_j_quant\n",
        "\n",
        "        err_j = (w_j - w_j_quant) / hinv_j\n",
        "        Err_block[:, j] = err_j\n",
        "        W_block[:, j:] -= err_j.unsqueeze(1).matmul(Hinv_block[j, j:].unsqueeze(0))\n",
        "\n",
        "    Q[:, i1:i2] = Q_block\n",
        "\n",
        "    W[:, i2:] -= Err_block.matmul(Hinv[i1:i2, i2:])\n",
        "\n",
        "  return Q, bitwidth"
      ],
      "metadata": {
        "id": "RAicKCL8B3Hq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def magnitude_pruning_and_gpqt_quantization(W_init, X, block_size=128, bitwidth=4, prune_rate=0.25):\n",
        "  W = W_init.clone()\n",
        "  Hinv = _compute_Hinv(X)\n",
        "  Q = torch.zeros_like(W)\n",
        "\n",
        "  n_cols = W.shape[1]\n",
        "  for i1 in range(0, n_cols, block_size):\n",
        "    i2 = min(i1 + block_size, n_cols)\n",
        "\n",
        "    W_block = W[:, i1:i2].clone()\n",
        "    Q_block = torch.zeros_like(W_block)\n",
        "    Err_block = torch.zeros_like(W_block)\n",
        "    Hinv_block = Hinv[i1:i2, i1:i2]\n",
        "\n",
        "    for j in range(i2 - i1):\n",
        "        w_j = W_block[:, j]\n",
        "        hinv_j = Hinv_block[j, j]\n",
        "\n",
        "        # quant\n",
        "        w_j_quant = _quant_tensor(w_j, bitwidth).flatten()\n",
        "\n",
        "        Q_block[:, j] = w_j_quant\n",
        "\n",
        "        err_j = (w_j - w_j_quant) / hinv_j\n",
        "        Err_block[:, j] = err_j\n",
        "        W_block[:, j:] -= err_j.unsqueeze(1).matmul(Hinv_block[j, j:].unsqueeze(0))\n",
        "\n",
        "    Q_block_prunned, _ = magnitude_pruning(Q_block, prune_rate=prune_rate)\n",
        "    Q[:, i1:i2] = Q_block_prunned\n",
        "\n",
        "    W[:, i2:] -= Err_block.matmul(Hinv[i1:i2, i2:])\n",
        "\n",
        "  return Q, bitwidth * (1 / prune_rate)"
      ],
      "metadata": {
        "id": "jx4lKDyNg5-W"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MASDoxyk9y-a"
      },
      "source": [
        "#### Пример расчета ошибки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "83_-ULNH9y-a",
        "outputId": "701ba421-4d17-4cca-dbad-52d1fbd40ff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "<ipython-input-22-b5cca9bf838d>:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
            "  l2 = ((W@X.T - WQ@X.T)**2).mean()\n",
            "100%|██████████| 7/7 [00:01<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(3.5155), tensor(1.4972), 0),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(158.4233), tensor(10.0173), 0),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(4.2363), tensor(1.6319), 0),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(4.8678), tensor(1.7560), 0),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(4.2832), tensor(1.6623), 0),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(11.6558), tensor(2.6132), 0),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(10.3696), tensor(2.4367), 0)}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#  Фкнкцию ниже можно переписать для своего удобства\n",
        "\n",
        "def test(your_quantization_function):\n",
        "    c = your_quantization_function\n",
        "    result = dict()\n",
        "    for name in tqdm(names):\n",
        "        X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
        "        W = torch.load(weight_paths[name])\n",
        "        X = X.float()\n",
        "        W = W.float()\n",
        "        WQ, compress_size = c(W, X)\n",
        "        l2 = ((W@X.T - WQ@X.T)**2).mean()\n",
        "        l1 = (W@X.T - WQ@X.T).abs().mean()\n",
        "        result[name] = (l2,l1, compress_size)\n",
        "    return  result\n",
        "\n",
        "\n",
        "def dummy_compress(X, *args):\n",
        "    X = torch.round(X)\n",
        "    compress_size = 0\n",
        "    return X, compress_size\n",
        "\n",
        "\n",
        "test(dummy_compress)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prunning"
      ],
      "metadata": {
        "id": "zyCFgTwNlKBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(magnitude_pruning, prune_rate=0.5)) # prune_rate x2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz9xSatKFkgD",
        "outputId": "c25ad604-b84f-4e1b-d597-adcd1ec407d9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [00:22<00:00,  3.22s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(0.2558), tensor(0.4042), 50.0),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(10.7266), tensor(2.6048), 50.0),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(0.2858), tensor(0.4271), 50.0),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(0.3829), tensor(0.4923), 50.0),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(0.2842), tensor(0.4236), 50.0),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(0.5403), tensor(0.5837), 50.0),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(0.5276), tensor(0.5807), 50.0)}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(magnitude_pruning, prune_rate=0.75)) # prune_rate x4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNdf-HJMgaM1",
        "outputId": "922ccb7f-5cb2-4c6a-9948-22028a33ff40"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [00:24<00:00,  3.52s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(0.9916), tensor(0.8000), 75.0),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(40.9887), tensor(5.1487), 75.0),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(1.1512), tensor(0.8521), 75.0),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(1.3904), tensor(0.9349), 75.0),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(1.1469), tensor(0.8606), 75.0),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(2.1815), tensor(1.1675), 75.0),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(2.0661), tensor(1.1415), 75.0)}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization"
      ],
      "metadata": {
        "id": "ThGT1SJ6lHMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(gpqt_quantization, bitwidth=8)) # int8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM8bAndnPf9A",
        "outputId": "36ca2fea-de8a-4554-f833-fe774d81b744"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [01:59<00:00, 17.04s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(1.8132e-07), tensor(0.0004), 8),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(8.1609e-07), tensor(0.0008), 8),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(3.6551e-07), tensor(0.0005), 8),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(2.4336e-07), tensor(0.0004), 8),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(3.3170e-07), tensor(0.0005), 8),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(5.5133e-07), tensor(0.0006), 8),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(5.1547e-07), tensor(0.0006), 8)}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(gpqt_quantization, bitwidth=4)) # int4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpP-yv1_gmKD",
        "outputId": "1e8fb588-f582-4b40-fe00-b469f046c4e0"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [01:55<00:00, 16.49s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(6.4006e-05), tensor(0.0068), 4),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(0.0003), tensor(0.0145), 4),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(9.8604e-05), tensor(0.0085), 4),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(8.9910e-05), tensor(0.0081), 4),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(0.0001), tensor(0.0099), 4),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(0.0001), tensor(0.0104), 4),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(0.0002), tensor(0.0110), 4)}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(gpqt_quantization, bitwidth=2)) # int4"
      ],
      "metadata": {
        "id": "GmAKPIDBkWTv",
        "outputId": "8a2a2194-3899-4c95-9e8a-a8e6e2cdfcf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [02:00<00:00, 17.16s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(0.0953), tensor(0.2595), 2),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(1.1166), tensor(0.9365), 2),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(0.1179), tensor(0.2889), 2),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(0.0917), tensor(0.2551), 2),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(0.0887), tensor(0.2365), 2),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(0.2321), tensor(0.4041), 2),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(0.2009), tensor(0.3771), 2)}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prunning and quantization"
      ],
      "metadata": {
        "id": "gajA7O-WlDDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(partial(magnitude_pruning_and_gpqt_quantization, bitwidth=4, prune_rate=0.5)) # int4 and prune_rate x2"
      ],
      "metadata": {
        "id": "YMbIWUsAh9MH",
        "outputId": "a53bec4a-d8c3-4926-8a8f-fd3f68795d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-22-b5cca9bf838d>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X = torch.load(\"./llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
            "<ipython-input-22-b5cca9bf838d>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  W = torch.load(weight_paths[name])\n",
            "100%|██████████| 7/7 [02:03<00:00, 17.67s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.layers.20.mlp.up_proj': (tensor(0.0884), tensor(0.2366), 8.0),\n",
              " 'model.layers.20.mlp.down_proj': (tensor(3.1419), tensor(1.4139), 8.0),\n",
              " 'model.layers.20.mlp.gate_proj': (tensor(0.0465), tensor(0.1728), 8.0),\n",
              " 'model.layers.20.self_attn.v_proj': (tensor(0.1512), tensor(0.3109), 8.0),\n",
              " 'model.layers.20.self_attn.o_proj': (tensor(0.0917), tensor(0.2424), 8.0),\n",
              " 'model.layers.20.self_attn.k_proj': (tensor(0.0389), tensor(0.1561), 8.0),\n",
              " 'model.layers.20.self_attn.q_proj': (tensor(0.0540), tensor(0.1841), 8.0)}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод\n",
        "\n",
        "- эффективнее всего работает gpqt_quantization, как в 8 бит, так и в 4 бит по сравнению с пруннигом, где сжатие уже в 2 раза (compress_size=50) уже сильно теряет в качестве\n",
        "- квантизация не падает драматически в качестве даже при квантизации в 2 бит\n",
        "- при этом комбинация пруннинг и квантизация работают (последний замер) почти также как и gpqt_quantization в отдельности"
      ],
      "metadata": {
        "id": "J992uPNRkOA5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "foKxRR81i73h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}